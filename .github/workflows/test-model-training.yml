name: Test Model Training

on:
  workflow_dispatch:
    inputs:
      prompt:
        description: 'Classification prompt'
        required: false
        default: 'Classify customer feedback as positive or negative sentiment'
      refinement_cycles:
        description: 'Number of refinement cycles'
        required: false
        default: '1'
      language:
        description: 'Language for training data'
        required: false
        default: 'english'
      mock_mode:
        description: 'Run in mock mode (skip actual training, test workflow only)'
        required: false
        default: 'false'
        type: choice
        options:
        - 'false'
        - 'true'

jobs:
  test-training:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Docker
      uses: docker/setup-buildx-action@v3
      
    - name: Pull WhiteLightning Docker image
      run: docker pull ghcr.io/whitelightning-ai/whitelightning:latest
      
    - name: Test Docker image
      run: |
        echo "üß™ Testing Docker image functionality..."
        docker run --rm ghcr.io/whitelightning-ai/whitelightning:latest --help || {
          echo "‚ùå Docker image test failed!"
          echo "üìã Available Docker images:"
          docker images
          exit 1
        }
        echo "‚úÖ Docker image is working correctly!"
      
    - name: Create models directory
      run: mkdir -p ./generated_models
      
    - name: Test model training
      env:
        OPEN_ROUTER_API_KEY: ${{ secrets.OPEN_ROUTER_API_KEY }}
      run: |
        echo "üîß Starting model training test..."
        echo "üìã Training parameters:"
        echo "  - Prompt: ${{ github.event.inputs.prompt || 'Classify customer feedback as positive or negative sentiment' }}"
        echo "  - Refinement cycles: ${{ github.event.inputs.refinement_cycles || '1' }}"
        echo "  - Language: ${{ github.event.inputs.language || 'english' }}"
        echo "  - API Key set: $([ -n "$OPEN_ROUTER_API_KEY" ] && echo 'Yes' || echo 'No')"
        echo ""
        
        # Check if API key is available
        if [ -z "$OPEN_ROUTER_API_KEY" ]; then
          echo "‚ùå OPEN_ROUTER_API_KEY is not set in GitHub secrets!"
          echo "Please add your OpenRouter API key as a repository secret."
          exit 1
        fi
        
        echo "üê≥ Running Docker container..."
        set -e  # Exit on any error
        
        docker run \
          --rm \
          -v $(pwd)/generated_models:/app/models \
          -e OPEN_ROUTER_API_KEY="$OPEN_ROUTER_API_KEY" \
          ghcr.io/whitelightning-ai/whitelightning:latest \
          -p="${{ github.event.inputs.prompt || 'Classify customer feedback as positive or negative sentiment' }}" \
          --refinement-cycles=${{ github.event.inputs.refinement_cycles || '1' }} \
          --generate-edge-cases="true" \
          --lang="${{ github.event.inputs.language || 'english' }}" \
        || {
          echo "‚ùå Docker command failed!"
          echo "üìã Debugging information:"
          echo "  - Current directory: $(pwd)"
          echo "  - Generated models directory exists: $([ -d './generated_models' ] && echo 'Yes' || echo 'No')"
          echo "  - Directory contents:"
          ls -la ./generated_models/ || echo "Directory doesn't exist or is empty"
          echo "  - Docker images:"
          docker images | grep whitelightning || echo "No WhiteLightning images found"
          exit 1
        }
        
        echo "‚úÖ Docker command completed successfully!"
    
    - name: Verify generated files
      run: |
        echo "Checking generated model files..."
        ls -la ./generated_models/
        
        # Find the actual model directory (could be sentiment_classifier or similar)
        MODEL_DIR=$(find ./generated_models -type d -name "*" | grep -v "^\./generated_models$" | head -1)
        if [ -z "$MODEL_DIR" ]; then
          echo "‚ùå No model subdirectory found"
          exit 1
        fi
        
        echo "üìÅ Found model directory: $MODEL_DIR"
        ls -la "$MODEL_DIR"
        
        # Check if required files exist in the model directory
        if [ ! -f "$MODEL_DIR/model.onnx" ]; then
          echo "‚ùå model.onnx not found in $MODEL_DIR"
          exit 1
        fi
        
        if [ ! -f "$MODEL_DIR/vocab.json" ]; then
          echo "‚ùå vocab.json not found in $MODEL_DIR"
          exit 1
        fi
        
        if [ ! -f "$MODEL_DIR/scaler.json" ]; then
          echo "‚ùå scaler.json not found in $MODEL_DIR"
          exit 1
        fi
        
        if [ ! -f "$MODEL_DIR/training_data.csv" ]; then
          echo "‚ùå training_data.csv not found in $MODEL_DIR"
          exit 1
        fi
        
        if [ ! -f "$MODEL_DIR/generation_config.json" ]; then
          echo "‚ùå generation_config.json not found in $MODEL_DIR"
          exit 1
        fi
        
        echo "‚úÖ All required files generated successfully!"
        
        # Show file sizes
        echo "üìä File sizes:"
        du -h "$MODEL_DIR"/*
    
    - name: Validate ONNX model
      run: |
        echo "Installing ONNX validation tools..."
        pip install onnx onnxruntime
        
        # Find the model directory
        MODEL_DIR=$(find ./generated_models -type d -name "*" | grep -v "^\./generated_models$" | head -1)
        echo "Using model directory: $MODEL_DIR"
        
        echo "Validating ONNX model..."
        python -c "
        import onnx
        import onnxruntime as ort
        import os
        
        model_dir = os.environ.get('MODEL_DIR', '$MODEL_DIR')
        
        # Load and validate ONNX model
        model = onnx.load(f'{model_dir}/model.onnx')
        onnx.checker.check_model(model)
        print('‚úÖ ONNX model structure is valid')
        
        # Test inference session
        session = ort.InferenceSession(f'{model_dir}/model.onnx')
        print('‚úÖ ONNX runtime can load the model')
        
        # Show model info
        print('üìã Model Input Info:')
        for input_meta in session.get_inputs():
            print(f'  - {input_meta.name}: {input_meta.type} {input_meta.shape}')
        
        print('üìã Model Output Info:')
        for output_meta in session.get_outputs():
            print(f'  - {output_meta.name}: {output_meta.type} {output_meta.shape}')
        "
      env:
        MODEL_DIR: ${{ env.MODEL_DIR }}
    
    - name: Test model inference
      run: |
        # Find the model directory
        MODEL_DIR=$(find ./generated_models -type d -name "*" | grep -v "^\./generated_models$" | head -1)
        echo "Using model directory: $MODEL_DIR"
        
        echo "Testing model inference with sample data..."
        python -c "
        import json
        import numpy as np
        import onnxruntime as ort
        from sklearn.feature_extraction.text import TfidfVectorizer
        import os
        
        model_dir = os.environ.get('MODEL_DIR', '$MODEL_DIR')
        
        # Load model components
        with open(f'{model_dir}/vocab.json', 'r') as f:
            vocab_data = json.load(f)
        
        with open(f'{model_dir}/scaler.json', 'r') as f:
            scaler_data = json.load(f)
        
        # Test with sample text
        test_text = 'This product is amazing! I love it so much!'
        
        # Create vectorizer with saved vocabulary
        vectorizer = TfidfVectorizer(vocabulary=vocab_data['vocab'])
        
        # Transform text
        X = vectorizer.transform([test_text]).toarray()
        
        # Apply scaling
        X_scaled = (X - np.array(scaler_data['mean'])) / np.array(scaler_data['scale'])
        
        # Run inference
        session = ort.InferenceSession(f'{model_dir}/model.onnx')
        input_name = session.get_inputs()[0].name
        
        result = session.run(None, {input_name: X_scaled.astype(np.float32)})
        prediction = result[0][0]
        
        print(f'‚úÖ Inference successful!')
        print(f'üìù Test text: {test_text}')
        print(f'üéØ Prediction: {prediction}')
        print(f'üìä Confidence: {max(prediction):.4f}')
        "
      env:
        MODEL_DIR: ${{ env.MODEL_DIR }}
    
    - name: Upload generated models as artifacts
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: generated-models-${{ github.run_number }}
        path: ./generated_models/
        retention-days: 7
    
    - name: Summary
      if: success()
      run: |
        echo "üéâ Model training test completed successfully!"
        echo "‚úÖ Docker image pulled and executed"
        echo "‚úÖ All required files generated"
        echo "‚úÖ ONNX model validated"
        echo "‚úÖ Inference test passed"
        echo "üì¶ Model artifacts uploaded" 